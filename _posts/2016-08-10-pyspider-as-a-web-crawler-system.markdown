---
layout: post
title:  "æ¼«è°ˆPyspiderç½‘ç»œçˆ¬è™«çš„å®è·µ"
subtitle:   ""
date:       2016-08-10
author:     "figotan"
header-img: "img/header/20160810.jpg"
header-mask: 0.5
catalog: true
tags:
    - ç½‘ç»œçˆ¬è™«
    - Pyspider
    - Python
    - 2016
---

æ„Ÿè§‰å¾ˆä¹…æ²¡æœ‰å†™ç‚¹ä¸œè¥¿äº†ï¼Œå› ä¸ºæœ€è¿‘å¤ªå¿™ï¼ˆå¤–å› ï¼‰æˆ–æ˜¯è‡ªèº«å¤ªæ‡’ï¼ˆå†…å› ï¼‰çš„åŸå› ã€‚æ€»ä¹‹ï¼Œå¾ˆæ—©ä¹‹å‰ï¼Œæˆ‘å°±å¼€å§‹è§„åˆ’ç€å†™ç‚¹å…³äºç½‘ç»œçˆ¬è™«æ–¹é¢çš„æ–‡ç« ï¼Œä»‹ç»æ€§è´¨çš„ï¼Œä½†æ›´é‡è¦çš„æ˜¯ï¼Œè®¡ç®—æœºä»¥åŠä¿¡æ¯ç§‘å­¦çš„å®è·µæ€§ï¼Œæ‰€ä»¥ï¼Œä»¥ä¸€ä¸ªå®å¹²è€…çš„è§’åº¦æ¥å†™ï¼Œæ›´ä¸ºåˆé€‚ä¸€äº›ã€‚  
åœ¨è¿™ä¹‹å‰ï¼Œè¿˜æ˜¯æœ‰å¿…è¦å¯¹ä¸€äº›æ¦‚å¿µæ€§çš„è¯æ±‡åšä¸€ä¸‹æ¢³ç†å’Œç§‘æ™®ï¼Œè‡³å°‘ï¼Œä¸ä¼šè®©è¯»è€…è§‰å¾—çªå…€æˆ–è€…ä¸€çŸ¥åŠè§£çš„è¯»ç€æµæ°´è´¦å¼çš„æ–‡å­—ã€‚

## ä»€ä¹ˆæ˜¯ç½‘ç»œçˆ¬è™«  
æ¥ä¸€æ®µé è°±çš„**ç»´åŸºç™¾ç§‘**çš„æƒå¨è§£é‡Š  

> ç½‘ç»œèœ˜è››ï¼ˆWeb spiderï¼‰ä¹Ÿå«ç½‘ç»œçˆ¬è™«ï¼ˆWeb crawlerï¼‰ï¼Œèš‚èšï¼ˆantï¼‰ï¼Œè‡ªåŠ¨æ£€ç´¢å·¥å…·ï¼ˆautomatic indexerï¼‰ï¼Œæˆ–è€…ï¼ˆåœ¨FOAFè½¯ä»¶æ¦‚å¿µä¸­ï¼‰ç½‘ç»œç–¾èµ°ï¼ˆWEB scutterï¼‰ï¼Œæ˜¯ä¸€ç§â€œè‡ªåŠ¨åŒ–æµè§ˆç½‘ç»œâ€çš„ç¨‹åºï¼Œæˆ–è€…è¯´æ˜¯ä¸€ç§ç½‘ç»œæœºå™¨äººã€‚å®ƒä»¬è¢«å¹¿æ³›ç”¨äºäº’è”ç½‘æœç´¢å¼•æ“æˆ–å…¶ä»–ç±»ä¼¼ç½‘ç«™ï¼Œä»¥è·å–æˆ–æ›´æ–°è¿™äº›ç½‘ç«™çš„å†…å®¹å’Œæ£€ç´¢æ–¹å¼ã€‚å®ƒä»¬å¯ä»¥è‡ªåŠ¨é‡‡é›†æ‰€æœ‰å…¶èƒ½å¤Ÿè®¿é—®åˆ°çš„é¡µé¢å†…å®¹ï¼Œä»¥ä¾›æœç´¢å¼•æ“åšè¿›ä¸€æ­¥å¤„ç†ï¼ˆåˆ†æ£€æ•´ç†ä¸‹è½½çš„é¡µé¢ï¼‰ï¼Œè€Œä½¿å¾—ç”¨æˆ·èƒ½æ›´å¿«çš„æ£€ç´¢åˆ°ä»–ä»¬éœ€è¦çš„ä¿¡æ¯ã€‚  

æ‰€ä»¥ç½‘ç»œçˆ¬è™«æ˜¯ä¸€ç§è‡ªåŠ¨åŒ–è¿è¡Œçš„è·å–ç›®æ ‡ç½‘é¡µä¿¡æ¯å†…å®¹å¹¶æŒ‰ä¸€å®šç»“æ„å­˜å‚¨çš„ä¾¿äºæŸ¥è¯¢æ£€ç´¢çš„è®¡ç®—æœºç¨‹åºã€‚

è¿™æ®µè¯ä¼¼ä¹è®©äººçœ‹äº†å¾ˆæ™•å¾ˆæ‡µé€¼ï¼Œè¿™ä¸ªä»€ä¹ˆé¬¼çš„åˆ°åº•èƒ½åšå•¥å‘¢ï¼Ÿ  
ä¸¾å‡ ä¸ªå¾ˆé€šä¿—çš„ä¾‹å­å§ï¼Œæ¯”å¦‚ä½ ä¸€æ‰‹æŒæ¡äº†å¾ˆå¤šçœ‹ç¾å¥³å›¾ç‰‡å’Œå°ç”µå½±çš„ç½‘ç«™å’Œè®ºå›ï¼ˆè¿™é‡Œéƒ½æŒ‡çš„çº¿ä¸Šçš„ï¼Œç½‘ç»œä¸Šçš„èµ„æºï¼‰ï¼Œæ¯å½“å¤œæ·±äººé™çš„æ—¶å€™ï¼Œä½ å·å·æ‰“å¼€ç”µè„‘ï¼Œç§’å¼€æµè§ˆå™¨é‚£ä¸ªåªæœ‰ä½ æœ¬äººçŸ¥é“çš„æ”¶è—å¤¹ï¼Œç„¶åä¸€é¡¿ç‚¹çœ‹æ’¸æ™ƒï¼Œä½†æ˜¯ï¼Œç­‰ç­‰ï¼Œå…¶å®çœŸå®çš„ä½“éªŒæ²¡è¿™ä¹ˆæµç•…ï¼Ÿ

* ä¿¡æ¯å¤ªæ•£äº†ï¼Œæ¯ä¸ªèµ„æºéƒ½éœ€è¦å•ç‹¬å»è®¿é—®ï¼Œæµªè´¹æ—¶é—´çš„ç‚¹å‡»
* èµ„æºè‰¯è ä¸é½ï¼Œå£å‘³ç¹æ‚ï¼Œéœ€è¦èŠ±æ—¶é—´æ‰¾åˆ°è‡ªå·±å–œæ¬¢çš„å†…å®¹
* å¤§é‡çƒ¦äººçš„å¹¿å‘Šï¼Œè¿™å°¼ç›æ˜¯å¹¿å‘Šè¿˜æ˜¯é©¬èµ›å…‹ï¼Ÿè¿˜æ˜¯æ’­æ”¾ï¼Ÿè¿˜æ˜¯ä¸‹è½½ï¼Ÿ
* æ‰‹åŠ¨æ‰¾æ›´æ–°ï¼Œåˆæ˜¯ä¸€ä»¶ç¹é‡çš„å·¥ä½œ

æ‰€ä»¥æƒ³çœ‹æ›´çº¯ç²¹æ›´ä¸“æ³¨çš„å†…å®¹ï¼Œè‡ªå·±åŠ¨æ‰‹ï¼Œå†™çˆ¬è™«å§ã€‚  

ä½ è¯´æˆ‘æ²¡è¿™éœ€æ±‚ï¼Œé‚£æ²¡å…³ç³»ï¼Œä½ æ€»éœ€è¦çœ‹æ–°é—»å§ï¼Œçœ‹æ–‡å­¦å§ï¼Œé€›ç”µå•†å§ï¼Œçœ‹è‚¡ç¥¨å§ï¼Œæ€»æœ‰é‚£ä¹ˆä¸€ç±»ä¿¡æ¯ï¼Œä½ ä¼šå…³æ³¨çš„ï¼Œä½†æ˜¯ç°æœ‰çš„æœåŠ¡æ€»æ˜¯ä¸å¤Ÿå•çº¯ï¼Œç•Œé¢ä¸å¤Ÿå‹å¥½ï¼Œä¿¡æ¯ä¸å¤Ÿçº¯ç²¹ï¼Œæ‰€ä»¥ï¼Œè¿™æ˜¯ä½ éœ€è¦å­¦ä¹ å¹¶å®è·µç½‘ç»œçˆ¬è™«æŠ€æœ¯çš„åŠ¨æœºã€‚

## ä¸ºä»€ä¹ˆæ˜¯Python  
å†™ç½‘ç»œçˆ¬è™«çš„è¯­è¨€æœ‰å¾ˆå¤šï¼Œç¼–ç¨‹çš„è¯­è¨€æ›´å¤šã€‚ä¸ªäººè®¤ä¸ºPythonæ˜¯ä¸€ç§å·¥å…·å‹çš„è¯­è¨€ï¼Œä¸Šæ‰‹å¿«ï¼Œè¯­æ³•ç®€å•ï¼ˆç›¸æ¯”äºC/C++/JAVAæ—ï¼‰ï¼Œå„ç§åŠŸèƒ½åº“ä¸°å¯Œè€Œä¸”å°å·§å•ä¸€ï¼ˆæ¯ä¸ªç‹¬ç«‹çš„åº“åªåšä¸€ä»¶äº‹æƒ…ï¼‰ï¼Œæ‰€ä»¥ç¼–ç¨‹å°±åƒæ˜¯åœ¨ç©ä¹é«˜ç§¯æœ¨ï¼Œç…§ç€è‡ªå·±è®¾è®¡å¥½çš„æµç¨‹ï¼Œæ‹¼æ¥å°±è¡Œäº†ã€‚å½“ç„¶ï¼Œè¿™æ˜¯ç¬”è€…ä¸ªäººçš„ç»éªŒå’Œå–œå¥½ã€‚å¦‚æœä½ æœ‰è‡ªå·±æ“…é•¿å¹¶å–œæ¬¢çš„ï¼Œå¤§å¯ç”¨è‡ªå·±çš„å»å®ç°ä¸€ä¸ªç½‘ç»œçˆ¬è™«ç³»ç»Ÿï¼Œè¿™ä¸ªä¸åœ¨æœ¬æ–‡çš„è®¨è®ºèŒƒå›´ä¹‹ç±»äº†ã€‚  
æœ‰å…³å‡ ç§ç¼–ç¨‹è¯­è¨€ç¼–å†™ç½‘ç»œçˆ¬è™«çš„æ¯”è¾ƒï¼Œå¯ä»¥å‚è€ƒçŸ¥ä¹ä¸Šçš„æ–‡ç«  [PHP, Python, Node.js å“ªä¸ªæ¯”è¾ƒé€‚åˆå†™çˆ¬è™«ï¼Ÿ](https://www.zhihu.com/question/23643061)

## ä¸ºä»€ä¹ˆæ˜¯Pyspider  
Pythonæœ‰å¾ˆå¤šæˆç†Ÿçš„ç½‘ç»œçˆ¬è™«æ¡†æ¶ï¼Œ çŸ¥ä¹ä¸Šå¾ˆå¤šå¤§ç‰›æ€»ç»“äº†ä¸€äº›å®è·µç»éªŒï¼Œå…·ä½“å¯ä»¥å‚è€ƒ[å¦‚ä½•å…¥é—¨ Python çˆ¬è™«ï¼Ÿ](https://www.zhihu.com/question/20899988)  
å¾ˆå¤šæ¨èç”¨requestsåšè¯·æ±‚ï¼Œqueryï¼soupåšé¡µé¢æ•°æ®(Html/Xml)è§£æï¼Œçœ‹èµ·æ¥å¾ˆçµæ´»ï¼Œç„¶è€Œï¼Œä¸€ä¸ªæ¯”è¾ƒå®Œå–„çš„ç½‘ç»œçˆ¬è™«ç³»ç»Ÿï¼Œæ‰€éœ€è¦æä¾›çš„åŠŸèƒ½å¯èƒ½è¿œè¿œä¸æ­¢è¿™äº›ã€‚ä¹Ÿæœ‰æ¨èScrapyçš„ï¼Œè™½ç„¶çœ‹èµ·æ¥åŠŸèƒ½éå¸¸å¼ºå¤§ï¼Œä½†æ˜¯è¿™ä¸ªæ¡†æ¶ä¸Šæ‰‹éœ€è¦ä¸€äº›æ—¶é—´ï¼Œæœ‰ä¸€å®šçš„å­¦ä¹ æˆæœ¬ï¼Œç›¸å¯¹äºæ–°æ‰‹æ¥è¯´ï¼Œå¾ˆéš¾å¿«é€Ÿä¸“æ³¨çˆ¬è™«ä¸šåŠ¡çš„å¼€å‘ã€‚  
Pyspideræ˜¯[Roy Binux](https://github.com/binux)å¼€å‘çš„ä¸€æ¬¾å¼€æºçš„ç½‘ç»œçˆ¬è™«ç³»ç»Ÿï¼Œå®ƒä¸æ­¢æ˜¯ä¸€ä¸ªçˆ¬è™«æ¡†æ¶ï¼Œè€Œæ˜¯ä¸€å¥—å®Œå¤‡çš„çˆ¬è™«ç³»ç»Ÿï¼Œä½¿ç”¨è¿™å¥—ç³»ç»Ÿä½ åªéœ€è¦å…³æ³¨ä¸¤ä»¶äº‹æƒ…

* ç›®æ ‡ç½‘ç«™ä¸Šçš„å†…å®¹å…ƒç´ çš„è§£æï¼Œè€Œä¸”åªéœ€è¦å…³æ³¨è§£æä»€ä¹ˆï¼Œè§£ææ¡†æ¶ä¹Ÿæœ‰æä¾›ï¼Œå¹¶ä¸”æä¾›äº†å¯è§†åŒ–å·¥å…·è¾…åŠ©ä»ç›®æ ‡é¡µé¢æŠ å–éœ€è¦è§£æçš„å…ƒç´ CSSå±æ€§
* è§£æå‡ºæ¥çš„å†…å®¹å…ƒç´ å¦‚ä½•ä¿å­˜ï¼Œä½ åªéœ€è¦å…³æ³¨æ•°æ®åº“è¡¨å­—æ®µçš„è®¾è®¡ï¼Œç„¶åæŠŠè§£æå‡ºæ¥çš„é¡µé¢å…ƒç´ å†…å®¹ä¿å­˜åˆ°æ•°æ®åº“è¡¨ä¸­
* é‚£ä¹ˆï¼Œå‰©ä¸‹çš„å‡ ä¹æ‰€æœ‰äº‹æƒ…ï¼Œå°±äº¤ç»™Pyspiderå§

æ˜¯ä¸æ˜¯å¬ä¸Šå»æ„Ÿè§‰å¾ˆç®€å•ï¼Œé‚£ä¹ˆï¼Œå¼€å§‹åŠ¨æ‰‹å§ï¼Œè·Ÿç€è¿™ç¯‡[å®˜æ–¹æ–‡æ¡£](http://docs.pyspider.org/en/latest/)ï¼Œæœ€å¿«å‡ åˆ†é’Ÿçš„åŠŸå¤«ï¼Œä½ å°±å¯ä»¥å­¦ä¼šä»2048ï¼ˆè‰æ¦´ï¼‰æ‰¾åˆ°çœŸçˆ±äº†ã€‚

ç®€å•çš„çˆ¬å–çœ‹å®˜æ–¹æ–‡æ¡£å°±å¯ä»¥äº†ï¼Œä¸è¿‡ï¼Œå®è·µè¿‡ç¨‹ä¸­æ€»ä¼šé‡åˆ°å„ç§é—®é¢˜ï¼Œé‚£ä¹ˆï¼Œçœ‹çœ‹è¿™äº›å¦‚ä½•è§£å†³çš„å§ã€‚

## å¦‚ä½•æ¨¡æ‹Ÿç™»é™†  
æœ‰äº›ç½‘ç«™å†…å®¹çš„å±•ç¤ºéœ€è¦ç”¨æˆ·ç™»å½•ï¼Œé‚£ä¹ˆå¦‚æœéœ€è¦çˆ¬å–è¿™æ ·çš„é¡µé¢å†…å®¹ï¼Œæˆ‘ä»¬çš„çˆ¬è™«å°±éœ€è¦æ¨¡æ‹Ÿç”¨æˆ·ç™»é™†ã€‚ç½‘ç«™ä¸€èˆ¬åœ¨é¡µé¢è·³è½¬æˆ–è€…åˆ·æ–°çš„æ—¶å€™ï¼Œä¹Ÿéœ€è¦è·å–ç™»å½•ä¿¡æ¯ä»¥ç¡®å®šè¿™ä¸ªé¡µé¢çš„è®¿é—®ç”¨æˆ·æ˜¯ç™»é™†è¿‡çš„ã€‚å¦‚æœæ¯æ¬¡éƒ½éœ€è¦ç”¨æˆ·é‡æ–°ç™»å½•ï¼Œé‚£ä¹ˆè¿™ç§ä½“éªŒå°±å¤ªçƒ‚äº†ï¼Œéœ€è¦ä¸€ç§æœºåˆ¶æŠŠä¹‹å‰ç”¨æˆ·ç™»é™†çš„ä¿¡æ¯ä¿å­˜èµ·æ¥ï¼Œè€Œä¸”ä¸€å®šæ˜¯ä¿å­˜åœ¨æµè§ˆå™¨å¯ä»¥è®¿é—®çš„æœ¬åœ°å­˜å‚¨ä¸Šï¼Œè¿™æ ·ï¼Œç”¨æˆ·åœ¨é¡µé¢è·³è½¬æˆ–è€…é¡µé¢åˆ·æ–°çš„æ—¶å€™ï¼Œç™»å½•ä¿¡æ¯è¢«ç½‘ç«™è‡ªåŠ¨è¯»å–ï¼Œå°±ä¸éœ€è¦ç”¨æˆ·é¢‘ç¹ç™»å½•äº†ã€‚è€Œè¿™ä¸ªä¿å­˜çš„åœ°æ–¹ï¼Œå«åšCookieã€‚  
çˆ¬è™«éœ€è¦åšçš„äº‹æƒ…ï¼Œä¸€æ˜¯æ¨¡æ‹Ÿç™»é™†ï¼Œæ‹¿åˆ°Cookieæ•°æ®ï¼Œç„¶åä¿å­˜ä¸‹æ¥ï¼ŒäºŒæ˜¯æ¯æ¬¡å»è®¿é—®ç½‘é¡µçš„æ—¶å€™ï¼Œå°†Cookieä¿¡æ¯ä¼ é€’ç»™è¯·æ±‚ï¼Œè¿™æ ·å°±å¯ä»¥æ­£å¸¸çˆ¬åˆ°éœ€è¦ç”¨æˆ·ç™»å½•çš„æ•°æ®äº†ã€‚

æˆ‘ä»¬å…ˆè®¾è®¡ä¸€ä¸ªç™»å½•ç±»ï¼Œç”¨æ¥ç®¡ç†ç™»å½•çš„è¯·æ±‚å’Œæ•°æ®

```
import urllib
import urllib2
import lxml.html as HTML

class Login(object):

    def __init__(self, username, password, login_url, post_url_prefix):
        self.username = username
        self.password = password
        self.login_url = login_url
        self.post_url_prefix = post_url_prefix

    def login(self):
        post_url, post_data = self.getPostData()
        post_url = self.post_url_prefix + post_url
        req = urllib2.Request(url = post_url, data = post_data)
        resp = urllib2.urlopen(req)
        return True

    def getPostData(self):
        url = self.login_url.strip()
        if not re.match(r'^http://', url):
            return None, None
        req = urllib2.Request(url)
        resp = urllib2.urlopen(req)
        login_page = resp.read()
        doc = HTML.fromstring (login_page)
        post_url = doc.xpath("//form[@method='post' and @id='lsform']/@action")[0]
        cookietime = doc.xpath("//input[@name='cookietime' and @id='ls_cookietime']/@value")[0]
        username = self.username
        password = self.password
        post_data = urllib.urlencode({
            'fastloginfield' : 'username',
            'username' : username,
            'password' : password,
            'quickforward' : 'no',
            'handlekey' : 'ls',
            'cookietime' : cookietime,
        })
        return post_url, post_data
```

ä»£ç è§£é‡Š  

* ç”¨æˆ·åusername, å¯†ç password, ç›®æ ‡ç½‘ç«™çš„ç™»å½•é¡µé¢åœ°å€login_url, ç›®æ ‡ç½‘ç«™çš„ä¸»åŸŸåpost_url_prefixï¼Œè¿™äº›å‚æ•°ä»å¤–éƒ¨ä¼ å…¥ï¼Œç›®æ ‡ç½‘ç«™çš„ç™»å½•é¡µé¢åœ°å€ä¹Ÿæœ‰å¯èƒ½å°±æ˜¯ç½‘ç«™çš„ä¸»é¡µåœ°å€ã€‚
* getPostDataé¦–å…ˆå‘ç›®æ ‡ç½‘ç«™çš„ç™»å½•é¡µé¢åœ°å€å‘èµ·ä¸€ä¸ªè¯·æ±‚ï¼Œç„¶åè§£æè¿™ä¸ªé¡µé¢çš„æ•°æ®ï¼Œè§£æå‡ºç™»å½•è¯·æ±‚çš„ç›®æ ‡åœ°å€å’Œpostè¯·æ±‚çš„æ•°æ®ï¼ˆç™»å½•è¯·æ±‚ä¸€èˆ¬ä¸ºpostè¯·æ±‚ï¼‰ï¼Œç„¶åè¿”å›è¿™ä¸¤ä¸ªå‚æ•°

è®¾è®¡ä¸€ä¸ªæ–¹æ³•ï¼Œè¿™ä¸ªæ–¹æ³•ç”¨æ¥è·å–çˆ¬å–ç½‘é¡µè¯·æ±‚éœ€è¦çš„Cookieæ•°æ®ã€‚

```
import os
import hashlib
import cookielib

LOGIN_URL = 'http://ç™»å½•é¡µé¢åœ°å€'
USER_NAME = 'ç”¨æˆ·å'
PASSWORD = 'å¯†ç '

HOST = 'ç›®æ ‡ç½‘é¡µä¸»åŸŸå'
REFERER = 'http://ç›®æ ‡ç½‘é¡µä¸»åŸŸå/'
POST_URL_PREFIX = 'http://ç›®æ ‡ç½‘é¡µä¸»åŸŸå/'

# !!! Notice !!!
# Tasks that share the same account MUST share the same cookies file
COOKIES_FILE = '/tmp/pyspider.%s.%s.cookies' % (HOST, hashlib.md5(USER_NAME).hexdigest())
COOKIES_DOMAIN = HOST

USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36'
HTTP_HEADERS = {
    'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Encoding' : 'gzip, deflate, sdch',
    'Accept-Language' : 'zh-CN,zh;q=0.8,en;q=0.6',
    'Connection' : 'keep-alive',
    'DNT' : '1',
    'Host' : HOST,
    'Referer' : REFERER,
    'User-Agent' : USER_AGENT,
}

def getCookies():
    cookiesJar = cookielib.MozillaCookieJar(COOKIES_FILE)
    if not os.path.isfile(COOKIES_FILE):
        cookiesJar.save()
    cookiesJar.load (COOKIES_FILE)
    cookieProcessor = urllib2.HTTPCookieProcessor(cookiesJar)
    cookieOpener = urllib2.build_opener(cookieProcessor, urllib2.HTTPHandler)
    for item in HTTP_HEADERS:
        cookieOpener.addheaders.append ((item ,HTTP_HEADERS[item]))
    urllib2.install_opener(cookieOpener)
    if len(cookiesJar) == 0:
        login = Login(USER_NAME, PASSWORD, LOGIN_URL, POST_URL_PREFIX)
        if login.login():
            cookiesJar.save()
        else:
            return None
    cookiesDict = {}
    for cookie in cookiesJar:
        if COOKIES_DOMAIN in cookie.domain:
            cookiesDict[cookie.name] = cookie.value
    return cookiesDict
```

ä»£ç è§£é‡Š

* USER_NAME PASSWORD LOGIN_URL POST_URL_PREFIX åˆ†åˆ«å®šä¹‰äº†ç”¨æˆ·åï¼å¯†ç ï¼ç™»å½•é¡µé¢åœ°å€ï¼ç›®æ ‡ç½‘é¡µå‰ç¼€
* å¦‚æœä»COOKIES_FILEè¯»å–å‡ºçš„Cookieä¿¡æ¯ä¸ºç©ºï¼Œé‚£ä¹ˆå°±è°ƒç”¨Loginåšç™»å½•æµç¨‹ï¼Œå¹¶ä¸”æŠŠè·å–åˆ°çš„ç»“æœä¿å­˜ï¼Œå¦‚æœCookieä¸ä¸ºç©ºï¼Œå°±è¿”å›Cookieä¿¡æ¯åˆ°å­—å…¸cookiesDictä¸­

Pyspideræ¯æ¬¡çˆ¬å–è¯·æ±‚éƒ½å¸¦ä¸ŠCookieå­—å…¸ï¼Œè¿™æ ·ï¼Œå‘ç›®æ ‡åœ°å€å‘è¯·æ±‚å°±å¯ä»¥è·å–åˆ°éœ€è¦ç™»å½•æ‰èƒ½è®¿é—®åˆ°çš„æ•°æ®äº†ã€‚

```
cookies = getCookies()
self.crawl(url, cookies = cookies, callback=self.index_page)
```

## å¦‚ä½•è§£æçˆ¬å–ä¸‹æ¥çš„å†…å®¹  
çˆ¬å–çš„å†…å®¹é€šè¿‡å›è°ƒçš„å‚æ•°**response**è¿”å›ï¼Œ**response**æœ‰å¤šç§è§£ææ–¹å¼  

* å¦‚æœè¿”å›çš„æ•°æ®æ˜¯jsonï¼Œåˆ™å¯ä»¥é€šè¿‡**response.json**è®¿é—®
* **response.doc**è¿”å›çš„æ˜¯**PyQuery**å¯¹è±¡
* **response.etree**è¿”å›çš„æ˜¯**lxml**å¯¹è±¡
* **response.text**è¿”å›çš„æ˜¯**unicode**æ–‡æœ¬
* **response.content**è¿”å›çš„æ˜¯**å­—èŠ‚ç **

æ‰€ä»¥è¿”å›æ•°æ®å¯ä»¥æ˜¯5ç§å½¢å¼ï¼Œunicodeå’Œå­—èŠ‚ç ä¸æ˜¯ç»“æ„åŒ–çš„æ•°æ®ï¼Œå¾ˆéš¾è§£æï¼Œè¿™é‡Œå°±ä¸èµ˜è¿°äº†ï¼Œjsonéœ€è¦ç‰¹å®šçš„æ¡ä»¶ï¼Œè€Œä¸”è§£æç›¸å¯¹ç®€å•ï¼Œä¹Ÿä¸å¿…è¯´ã€‚  
å¸¸ç”¨çš„å°±æ˜¯PyQueryå’Œlxmlçš„æ–¹å¼ï¼Œå…³äºlxmlï¼Œå¯ä»¥é‡‡ç”¨XPathçš„è¯­æ³•æ¥è§£æï¼Œæ¯”å¦‚å‰é¢æ¨¡æ‹Ÿç™»å½•ä¸­å°±é‡‡ç”¨äº†xpathçš„è¯­æ³•è§£æç½‘é¡µï¼Œå…·ä½“å¯å‚è€ƒlxmlå’ŒXPathçš„[ç›¸å…³æ–‡æ¡£](http://www.w3schools.com/xsl/xpath_syntax.asp)ã€‚  

XPathé€‰æ‹©å™¨å‚è€ƒ

|é€‰æ‹©å™¨|ç¤ºä¾‹|ç¤ºä¾‹è¯´æ˜|
|:-------:|:-------|:-------|
|nodename|bookstore|é€‰æ‹©æ‰€æœ‰åç§°å«åš"bookstore"çš„èŠ‚ç‚¹|
|/|bookstore/book|é€‰æ‹©"bookstore"çš„èŠ‚ç‚¹çš„æ‰€æœ‰"book"å­èŠ‚ç‚¹|
|//|//book|é€‰æ‹©æ–‡æ¡£ä¸­æ‰€æœ‰åç§°å«åš"book"çš„èŠ‚ç‚¹ï¼Œä¸ç®¡å®ƒä»¬çš„çˆ¶èŠ‚ç‚¹å«åšä»€ä¹ˆ|
|.||é€‰æ‹©å½“å‰çš„èŠ‚ç‚¹|
|..||é€‰æ‹©å½“å‰èŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹|
|@|//@lang|é€‰æ‹©æ‰€æœ‰åç§°å«åš"lang"çš„å±æ€§|
||bookstore//book|é€‰æ‹©èŠ‚ç‚¹"bookstore"æ‰€æœ‰å«åš"book"çš„å­å­™èŠ‚ç‚¹ï¼Œbookstoreä¸ä¸€å®šæ˜¯bookçš„çˆ¶èŠ‚ç‚¹|
||/bookstore/book[1]|é€‰æ‹©èŠ‚ç‚¹"bookstore"çš„ç¬¬ä¸€ä¸ªå«åš"book"çš„å­èŠ‚ç‚¹|
||/bookstore/book[last()]|é€‰æ‹©èŠ‚ç‚¹"bookstore"çš„æœ€åä¸€ä¸ªå«åš"book"çš„å­èŠ‚ç‚¹|
||//title[@lang]|é€‰æ‹©æ‰€æœ‰æœ‰ä¸€ä¸ªå±æ€§åå«åš"lang"çš„titleèŠ‚ç‚¹|
||//title[@lang='en']|é€‰æ‹©æ‰€æœ‰æœ‰ä¸€ä¸ªå±æ€§"lang"çš„å€¼ä¸º"en"çš„titleèŠ‚ç‚¹|
|*|/bookstore/*|é€‰æ‹©"bookstore"èŠ‚ç‚¹çš„æ‰€æœ‰å­èŠ‚ç‚¹|
||//*|é€‰æ‹©æ–‡æ¡£ä¸­æ‰€æœ‰çš„èŠ‚ç‚¹|
|@*|//title[@*]|é€‰æ‹©æ‰€æœ‰çš„"title"èŠ‚ç‚¹è‡³å°‘å«æœ‰ä¸€ä¸ªå±æ€§ï¼Œå±æ€§åç§°ä¸é™|

PyQueryå¯ä»¥é‡‡ç”¨**CSSé€‰æ‹©å™¨**ä½œä¸ºå‚æ•°å¯¹ç½‘é¡µè¿›è¡Œè§£æã€‚  
ç±»ä¼¼è¿™æ ·

```
response.doc('.ml.mlt.mtw.cl > li').items()
```

æˆ–è€…è¿™æ ·

```
response.doc('.pti > .pdbt > .authi > em > span').attr('title')
```

å…³äºPyQueryæ›´å¤šç©æ³•ï¼Œå¯ä»¥å‚è€ƒ[PyQuery complete API](https://pythonhosted.org/pyquery/api.html)  

CSSé€‰æ‹©å™¨  

|é€‰æ‹©å™¨|ç¤ºä¾‹|ç¤ºä¾‹è¯´æ˜|
|:-------:|:-------|:-------|
|.class|.intro|Selects all elements with class="intro"|
|#id|#firstname|Selects the element with id="firstname"|
|element|p|Selects all \<p\> elements|
|element,element|div, p|Selects all <div> elements and all <p> elements|
|element element|div p|Selects all \<p\> elements inside \<div\> elements|
|element>element|div > p|Selects all \<p\> elements where the parent is a \<div\> element|
|[attribute]|[target]|Selects all elements with a target attribute|
|[attribute=value]|[target=_blank]|Selects all elements with target="_blank"|
|[attribute^=value]|a[href^="https"]|Selects every \<a\> element whose href attribute value begins with "https"|
|[attribute$=value]|a[href$=".pdf"]|Selects every \<a\> element whose href attribute value ends with ".pdf"|
|[attribute*=value]|a[href*="w3schools"]|Selects every \<a\> element whose href attribute value contains the substring "w3schools"|
|:checked|input:checked|Selects every checked \<input\> element|

æ›´å¤šè¯¦æƒ…è¯·å‚è€ƒ[CSS Selector Reference](http://www.w3schools.com/cssref/css_selectors.asp)

## å¦‚ä½•å°†æ•°æ®ä¿å­˜åˆ°MySQLä¸­  
å°†MySQLçš„æ•°æ®åº“è®¿é—®å°è£…æˆä¸€ä¸ªç±»

```
import hashlib
import unicodedata
import mysql.connector
from mysql.connector import errorcode

class MySQLDB:
    
    username = 'æ•°æ®åº“ç”¨æˆ·å'
    password = 'æ•°æ®åº“å¯†ç '
    database = 'æ•°æ®åº“å'
    host = 'localhost'    #æ•°æ®åº“ä¸»æœºåœ°å€
    connection = ''
    isconnect = True
    placeholder = '%s'
    
    def __init__(self):
        if self.isconnect:
            MySQLDB.connect(self)
            MySQLDB.initdb(self)
    
    def escape(self,string):
        return '`%s`' % string
    
    def connect(self):
        config = {
            'user':self.username,
            'password':self.password,
            'host':self.host
        }
        
        if self.database != None:
            config['database'] = self.database
    
        try:
            cnx = mysql.connector.connect(**config)
            self.connection = cnx
            return True
        except mysql.connector.Error as err:
            if (err.errno == errorcode.ER_ACCESS_DENIED_ERROR):
                print "The credentials you provided are not correct."
            elif (err.errno == errorcode.ER_BAD_DB_ERROR):
                print "The database you provided does not exist."
            else:
                print "Something went wrong: " , err
            return False
        
    def initdb(self):
        if self.connection == '':
            print "Please connect first"
            return False
        cursor = self.connection.cursor()
        
        # åˆ›å»ºè¡¨çš„å®šä¹‰
        sql = 'CREATE TABLE IF NOT EXISTS \
        table_name ( \
        id VARCHAR(64) PRIMARY KEY, \
        url TEXT, \
        title TEXT, \
        type TEXT, \
        thumb TEXT, \
        count INTEGER, \
        images TEXT, \
        tags TEXT, \
        post_time DATETIME \
        ) ENGINE=INNODB DEFAULT CHARSET=UTF8'
        try:
            cursor.execute(sql)
            self.connection.commit()
            return True
        except mysql.connector.Error as err:
            print ("An error occured: {}".format(err))
            return False
        
    def cleardb (self):
        if self.connection == '':
            print "Please connect first"
            return False
        cursor = self.connection.cursor()
        sql = 'DROP TABLE IF EXISTS table_name'
        try:
            cursor.execute(sql)
            self.connection.commit()
            return True
        except mysql.connector.Error as err:
            print ("An error occured: {}".format(err))
            return False
        
    def insert (self,**values):
        if self.connection == '':
            print "Please connect first"
            return False
        cursor = self.connection.cursor()
        # æ’å…¥æ•°æ®
        sql = "insert into table_name (id, url, title, type, thumb, count, temperature, images, tags, post_time) values (%s,%s,%s,%s,%s,%s,%s,%s,%s) on duplicate key update id=VALUES(id), url=VALUES(url), title=VALUES(title), type=VALUES(type), thumb=VALUES(thumb), count=VALUES(count), images=VALUES(images), tags=VALUES(tags), post_time=VALUES(post_time)"
        title = unicodedata.normalize('NFKD', values['title']).encode('ascii','ignore')
        images = ", ".join('%s' % k for k in values['images'])
        params = (hashlib.md5(title + images).hexdigest(), values['url'], values['title'], values['type'], values['thumb'], values['count'], images, '', values['date'])
        try:
            cursor.execute(sql,params)
            self.connection.commit()
            return True
        except mysql.connector.Error as err:
            print ("An error occured: {}".format(err))
            return False
    
    def replace(self,tablename=None,**values):
        if self.connection == '':
            print "Please connect first"
            return False
    
        tablename = self.escape(tablename)
        if values:
            _keys = ", ".join(self.escape(k) for k in values)
            _values = ", ".join([self.placeholder, ] * len(values))
            sql_query = "REPLACE INTO %s (%s) VALUES (%s)" % (tablename, _keys, _values)
        else:
            sql_query = "REPLACE INTO %s DEFAULT VALUES" % tablename
                
        cur = self.connection.cursor()
        try:
            if values:
                cur.execute(sql_query, list(itervalues(values)))
            else:
                cur.execute(sql_query)
            self.connection.commit()
            return True
        except mysql.connector.Error as err:
            print ("An error occured: {}".format(err))
            return False
```

åœ¨å¤„ç†çˆ¬å–ç»“æœçš„å›è°ƒä¸­ä¿å­˜åˆ°æ•°æ®åº“

```
def on_result(self, result):
        db = MySQLDB()
        db.insert(**result)
```

## å¦‚ä½•åœ¨çˆ¬è™«è„šæœ¬æ›´æ–°åé‡æ–°è¿è¡Œä¹‹å‰æ‰§è¡Œè¿‡çš„ä»»åŠ¡  
æ¯”å¦‚è¿™ç§åœºæ™¯ï¼Œçˆ¬å–äº†ä¸€äº›æ•°æ®ï¼Œå‘ç°æ²¡æœ‰å†™ä¿å­˜åˆ°æ•°æ®åº“çš„é€»è¾‘ï¼Œç„¶ååŠ ä¸Šäº†è¿™æ®µé€»è¾‘ï¼Œå´å‘ç°ä¹‹å‰è·‘è¿‡çš„ä»»åŠ¡ä¸ä¼šåœ¨æ‰§è¡Œäº†ã€‚é‚£ä¹ˆå¦‚ä½•åšåˆ°åœ¨çˆ¬è™«è„šæœ¬æ”¹åŠ¨åï¼Œä¹‹å‰çš„ä»»åŠ¡é‡æ–°è‡ªåŠ¨å†è·‘ä¸€éå‘¢ã€‚  
åœ¨**crawl_config**ä¸­ä½¿ç”¨**itag**æ¥æ ‡ç¤ºçˆ¬è™«è„šæœ¬çš„ç‰ˆæœ¬å·ï¼Œå¦‚æœè¿™ä¸ªå€¼å‘ç”Ÿæ”¹å˜ï¼Œé‚£ä¹ˆæ‰€æœ‰çš„ä»»åŠ¡éƒ½ä¼šé‡æ–°å†è·‘ä¸€éã€‚ç¤ºä¾‹ä»£ç å¦‚ä¸‹

```
class Handler(BaseHandler):
    crawl_config = {
        'headers': {
            'User-Agent': USER_AGENT,
        },
        'itag': 'v1'
    }
```

**itag**ä¹Ÿå¯ä»¥ç”¨æ¥æ§åˆ¶ç‰¹å®šçš„ä»»åŠ¡æ˜¯å¦éœ€è¦é‡æ–°æ‰§è¡Œï¼Œè¯¦è§[å®˜æ–¹æ–‡æ¡£](http://docs.pyspider.org/en/latest/apis/self.crawl/#itag)ã€‚

## å¦‚ä½•è§£æJavaScriptä»£ç 
å…·ä½“å¦‚ä½•ä½¿ç”¨çš„å¯ä»¥çœ‹å®˜æ–¹æ–‡æ¡£ï¼Œè¿™é‡Œåˆ—ä¸¾å‡ºä¸€äº›å¯ä¾›å‚è€ƒçš„JavaScriptè§£æå™¨  
åŸºäº**Webkit**çš„[PhantomJS](http://phantomjs.org/)
åŸºäº**Gecko**çš„[SlimerJS](http://slimerjs.org/)  
åŸºäº**PhantomJS**å’Œ**SlimerJS**çš„[CasperJS](http://casperjs.org/)  
[Nightmare](http://www.nightmarejs.org/)  
[Selenium](http://www.seleniumhq.org/)  
[spynner](https://github.com/makinacorpus/spynner)  
[ghost.py](https://github.com/jeanphix/Ghost.py)  

æ›´å¤šå·¥å…·ï¼æ¡†æ¶è¯·å‚è€ƒ[Headless Browser and scraping - solutions](http://stackoverflow.com/questions/18539491/headless-browser-and-scraping-solutions)

## å‚è€ƒèµ„æ–™  

[binux/pyspider](https://github.com/binux/pyspider)  
[Pyspiderå®˜æ–¹æ–‡æ¡£](http://docs.pyspider.org/en/latest/)  
[pyspideræ¶æ„è®¾è®¡](http://blog.binux.me/2014/02/pyspider-architecture/)  
[pyspiderä¸­æ–‡è„šæœ¬ç¼–å†™æŒ‡å—](http://www.jishubu.net/yunwei/python/411.html)  
[Pyspiderçˆ¬è™«æ•™ç¨‹](http://www.cnblogs.com/panliu/p/4524157.html)  
[æŠŠ pyspiderçš„ç»“æœå­˜å…¥è‡ªå®šä¹‰çš„mysqlæ•°æ®åº“ä¸­](http://www.jishubu.net/yunwei/python/424.html)  
[pyspiderçš„mysqlæ•°æ®å­˜å‚¨æ¥å£](http://blog.csdn.net/u012293522/article/details/44222207)  
[PyQuery complete API](https://pythonhosted.org/pyquery/api.html)  
[CSS Selector Reference](http://www.w3schools.com/cssref/css_selectors.asp)  

## æ”¶é›†çš„ä¸€äº›å…¶å®ƒç½‘ç»œçˆ¬è™«çš„èµ„æ–™  

#### Java
[é›ªçƒè‚¡ç¥¨ä¿¡æ¯è¶…çº§çˆ¬è™«](https://github.com/decaywood/XueQiuSuperSpider)  
[ä¸€ä¸ªç®€å•æ˜“ç”¨çš„çˆ¬è™«æ¡†æ¶,å†…ç½®ä»£ç†ç®¡ç†æ¨¡å—,çµæ´»è®¾ç½®å¤šçº¿ç¨‹çˆ¬å–](https://github.com/MrJiao/SpiderJackson)  
[A scalable web crawler framework for Java](https://github.com/code4craft/webmagic)  
[å¼ºåŠ› Java çˆ¬è™«ï¼Œåˆ—è¡¨åˆ†é¡µã€è¯¦ç»†é¡µåˆ†é¡µã€ajaxã€å¾®å†…æ ¸é«˜æ‰©å±•ã€é…ç½®çµæ´»](https://git.oschina.net/l-weiwei/Spiderman2)  
[åˆ†å¸ƒå¼çš„é€šç”¨çˆ¬è™«ï¼Œå¯ä»¥çƒ­æ’æ‹”å„ä¸ªç»„ä»¶ï¼ˆæä¾›é»˜è®¤çš„ï¼‰ï¼Œè‡ªåŠ¨åˆ‡æ¢ä»£ç†ï¼Œè‡ªåŠ¨ç»“æ„åŒ–æ•°æ®ä¸å­˜å‚¨ã€‚ä½¿ç”¨redisï¼Œåˆ†å¸ƒå¼è°ƒåº¦ç­‰æŠ€æœ¯](https://github.com/xjtushilei/ScriptSpider)  

#### Python
[Scrapy](http://scrapy.org/)  
[a smart stream-like crawler & etl python library](https://github.com/ferventdesert/etlpy)  
[çˆ¬è§†é¢‘éŸ³é¢‘ç¥å™¨You-Get](https://you-get.org/)  
[å¦ä¸€æ¬¾è§†é¢‘ä¸‹è½½ç¥å™¨youtube-dl](https://github.com/rg3/youtube-dl)  
[youtube-dlå›¾å½¢ç•Œé¢ç‰ˆ](https://github.com/MrS0m30n3/youtube-dl-gui)  
[è‡ªåŠ¨æŠ“å–TumblræŒ‡å®šç”¨æˆ·è§†é¢‘åˆ†äº«](https://github.com/Thoxvi/MyCar_python)  
[crawley](https://github.com/jmg/crawley)  
[ä¹Œäº‘å…¬å¼€æ¼æ´ã€çŸ¥è¯†åº“çˆ¬è™«å’Œæœç´¢](https://github.com/hanc00l/wooyun_public)  
[ä¸‹è½½æŒ‡å®šçš„ Tumblr åšå®¢ä¸­çš„å›¾ç‰‡ï¼Œè§†é¢‘](https://github.com/dixudx/tumblr-crawler)  
[ä¸‹è½½æŒ‡å®šçš„ Tumblr åšå®¢ä¸­çš„å›¾ç‰‡ï¼Œè§†é¢‘ï¼Œç„é­‚ä¿®æ”¹ç‰ˆ](https://github.com/xuanhun/tumblr-crawler)  
[DHTç½‘ç»œçˆ¬è™«](https://github.com/78/ssbc)  
[è±†ç“£ç”µå½±ã€ä¹¦ç±ã€å°ç»„ã€ç›¸å†Œã€ä¸œè¥¿ç­‰çˆ¬è™«é›† writen in Python](https://github.com/dontcontactme/doubanspiders)  
[å¦‚ä½•ä¸ç”¨å®¢æˆ·ç«¯ä¸‹è½½ YouKu è§†é¢‘-YouKu å®ç°ä¸‹è½½ Python3 å®ç°](http://zpz.name/2378/)  
[ä¸€ä¸ªå¯é…ç½®çš„ã€åˆ†å¸ƒå¼çš„çˆ¬è™«æ¡†æ¶](https://github.com/yijingping/unicrawler)  
[cloud-based web crawling platform](https://github.com/scrapinghub)  
[ç™¾åº¦äº‘çˆ¬è™«-çˆ¬å–ç™¾åº¦äº‘/ç™¾åº¦ç½‘ç›˜æ‰€æœ‰çš„åˆ†äº«æ–‡ä»¶](https://github.com/x-spiders/baiduyun-spider)  
[çˆ±ä¸APPå›¾ç‰‡çˆ¬è™«ï¼Œä»¥åŠå…æ”¯ä»˜ç ´è§£VIPçœ‹å›¾](https://github.com/x-spiders/aiss-spider)  
[å¾®ä¿¡å…¬ä¼—å·çˆ¬è™«](https://github.com/bowenpay/wechat-spider)  
[æ‹‰å‹¾ç½‘çˆ¬è™«](https://github.com/whatsGhost/lagou_spider)  
[ç™¾åº¦ç½‘ç›˜çˆ¬è™«ï¼ˆå¦‚ä½•çˆ¬å–ç™¾åº¦ç½‘ç›˜ï¼‰](https://www.v2ex.com/t/348731#reply7)  
[scrapyçˆ¬è™«å›¾å½¢ç®¡ç†ç•Œé¢](https://github.com/DormyMo/SpiderKeeper)  
[PornHubBot - ğŸ” å…¨çƒæœ€å¤§æˆäººç½‘ç«™ PornHub çˆ¬è™« ï¼ˆScrapyã€MongoDBï¼‰ ä¸€å¤© 500w çš„æµ·é‡æ•°æ®](https://github.com/xiyouMc/WebHubBot)  

#### PHP
[PHP Crawler](https://sourceforge.net/projects/php-crawler/)  
[PHPCrawl](https://sourceforge.net/projects/phpcrawl/)  
[Phpfetcher](https://github.com/fanfank/phpfetcher)  
[php spider framework](https://github.com/hirudy/Tspider)  
[æˆ‘ç”¨çˆ¬è™«ä¸€å¤©æ—¶é—´â€œå·äº†â€çŸ¥ä¹ä¸€ç™¾ä¸‡ç”¨æˆ·ï¼Œåªä¸ºè¯æ˜PHPæ˜¯ä¸–ç•Œä¸Šæœ€å¥½çš„è¯­è¨€](https://github.com/owner888/phpspider)  
[çˆ¬è™«ç»„ä»¶](https://github.com/slince/spider)  
[PHP Simple HTML DOM Parser](http://simplehtmldom.sourceforge.net)  
[QueryList](https://querylist.cc)  
[Goutte, a simple PHP Web Scraper](https://github.com/FriendsOfPHP/Goutte)  

#### Nodejs
[Nodejs ç¼–å†™çš„çˆ¬è™«å·¥å…·](https://github.com/xwartz/spider)  
[æ‰¹é‡æŠ“å–AVç£é“¾æˆ–å°é¢çš„è‹¦åŠ³åŠ›](https://github.com/raawaa/jav-scrapy)  
[Easily download all the photos from a Tumblr blog.](https://github.com/Leeiio/tumblr-downloader)  
[DHT Spider + BitTorrent Client = P2P Spider](https://github.com/dontcontactme/p2pspider)  
[P2P Spiderä¿®æ”¹ç‰ˆ,æ·»åŠ äº†babelï¼Œeslintï¼Œgulpç­‰å·¥å…·æ¥æ”¯æŒes6ä»£ç ](https://github.com/callmelanmao/btspider)  
[ä¸€ä¸ªNode.jsç¦åˆ©å›¾ç½‘ç«™çˆ¬è™«ç¨‹åº](https://github.com/nieheyong/HanhandeSpider)  
[ä¸€ä¸ªç®€å•çš„dhtçˆ¬è™«ï¼Œç”¨äºæœé›†infohash](https://github.com/beilunyang/dhtCrawler)  
[ç™¾åº¦äº‘åˆ†äº«çˆ¬è™«é¡¹ç›®](https://github.com/callmelanmao/yunshare)  

#### Ruby
[A simple DHT crawler, written in Ruby](https://github.com/dontcontactme/rbdht)  

#### C sharp
[visualized crawler & ETL IDE written with C#/WPF](https://github.com/ferventdesert/Hawk)  

#### Erlang
[ä½¿ç”¨erlangå®ç°P2Pç£åŠ›æœç´¢](https://github.com/kevinlynx/dhtcrawler2)  

#### C++

[ç»™ä¸äº†ä½ æ¢¦ä¸­æƒ…äººï¼Œè‡³å°‘è¿˜æœ‰ç¡¬ç›˜å¥³ç¥ï¼šhardseed](https://github.com/yangyangwithgnu/hardseed)  

#### Golang
[a distributed, high concurrency and powerful web crawler software](https://github.com/henrylee2cn/pholcus)  

#### ç½‘ç»œçˆ¬è™«ä¸“é¢˜
[open-open ç½‘ç»œçˆ¬è™«ä¸“é¢˜](http://www.open-open.com/lib/list/96)  
[ä½ æƒ³è¦çš„çˆ¬è™«ï¼Œéƒ½åœ¨è¿™é‡Œ](https://github.com/x-spiders)